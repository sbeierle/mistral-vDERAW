import torch
from safetensors.torch import load_file, save_file
from transformers import AutoTokenizer

# ğŸ“¦ Konfiguration
model_path = "./mistral/model-00003-of-00003.safetensors"
tensor_key = "lm_head.weight"  # Standard fÃ¼r Mistral GGUF/FT-Modelle
tokenizer_path = "./mistral/"
target_norm = 15.0              # ğŸ” Anpassbar (z.â€¯B. 20.0 zu stark, 0.7 Scale zu schwach)

# ğŸ“˜ Kritische Phrasen (nur exemplarisch!)
phrases = [
    "it is strictly forbidden",
    "as an AI developed by",
    "i am not allowed to do that",
    "this may be considered illegal",
    "we do not condone unethical behavior"
]

# ğŸš€ Tokenizer laden (lokal)
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)

# ğŸ“¥ Modell laden
print(f"ğŸ“¦ Lade Datei: {model_path}")
weights = load_file(model_path)

if tensor_key not in weights:
    raise ValueError(f"âŒ {tensor_key} nicht gefunden!")

tensor = weights[tensor_key]
patched = set()

# ğŸ›  Patch-Logik
for phrase in phrases:
    token_ids = tokenizer.encode(phrase, add_special_tokens=False)
    print(f"\nğŸ“Œ Phrase: '{phrase}' â†’ Token-IDs: {token_ids}")

    for token_id in token_ids:
        if token_id < 4096:  # â›” StrukturwÃ¶rter auslassen
            print(f"   âš ï¸  Token {token_id} ist Strukturwort â†’ Ã¼bersprungen")
            continue
        if token_id in patched:
            continue
        vec = tensor[token_id]
        norm_before = torch.norm(vec, p=2).item()
        tensor[token_id] = vec / norm_before * target_norm
        norm_after = torch.norm(tensor[token_id], p=2).item()
        print(f"   ğŸ”§ Token {token_id:<6} Norm: {norm_before:.6f} â†’ {norm_after:.6f}")
        patched.add(token_id)

# ğŸ’¾ Speichern
save_file(weights, model_path)
print(f"\nâœ… Gruppenpatch abgeschlossen â†’ Datei Ã¼berschrieben: {model_path}")
